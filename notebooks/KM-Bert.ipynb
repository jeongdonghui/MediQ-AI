{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a77394c-9366-4396-af9a-2677b78d9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# transformersê°€ TensorFlow/Flax ìª½ì„ ì•„ì˜ˆ ë¬´ì‹œí•˜ê²Œ ê°•ì œ ì„¤ì •\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"   # ì¶”ê°€ ì•ˆì „ì¥ì¹˜\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # TF ê´€ë ¨ ê²½ê³  ìˆ¨ê¸°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5ee4f8-ff05-41bc-8051-dba342bd1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Python Version ===\n",
      "3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "âœ” torch already installed\n",
      "âœ” transformers already installed\n",
      "âœ” accelerate already installed\n",
      "âœ” datasets already installed\n",
      "ğŸ“¦ Installing scikit-learn ...\n",
      "=== íŒ¨í‚¤ì§€ ì²´í¬ ì™„ë£Œ ===\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    print(f\"ğŸ“¦ Installing {pkg} ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# -------------------------\n",
    "# 1) ë²„ì „ í™•ì¸\n",
    "# -------------------------\n",
    "\n",
    "print(\"=== Python Version ===\")\n",
    "print(sys.version)\n",
    "\n",
    "# -------------------------\n",
    "# 2) íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ + ìë™ ì„¤ì¹˜\n",
    "# -------------------------\n",
    "\n",
    "required = [\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\",\n",
    "    \"scikit-learn\"\n",
    "]\n",
    "\n",
    "for pkg in required:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"âœ” {pkg} already installed\")\n",
    "    except ModuleNotFoundError:\n",
    "        pip_install(pkg)\n",
    "\n",
    "print(\"=== íŒ¨í‚¤ì§€ ì²´í¬ ì™„ë£Œ ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73048adf-ec7d-4cb4-a557-04f6f0a20c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.3.1\n",
      "Transformers: 4.39.3\n",
      "Accelerate: 0.30.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, accelerate\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51406389-fd08-40ab-b628-b6cb16729cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KM-BERT tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jang8\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([1, 10])\n",
      "last_hidden_state shape: torch.Size([1, 10, 768])\n",
      "ğŸ‰ KM-BERT forward pass ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"madatnlp/km-bert\"\n",
    "\n",
    "print(\"Loading KM-BERT tokenizer/model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"input_ids shape:\", inputs[\"input_ids\"].shape)\n",
    "print(\"last_hidden_state shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"ğŸ‰ KM-BERT forward pass ì„±ê³µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35863762-2fdd-49e6-bd6c-07cc5f3a425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          colloquial normalized_symptom department\n",
      "0    ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "1   í•œìª½ ë¨¸ë¦¬ê°€ ì§€ëˆì§€ëˆ ì‘¤ì…”ìš”.                í¸ë‘í†µ        ì‹ ê²½ê³¼\n",
      "2  ë¨¸ë¦¬ ì „ì²´ê°€ ì¡°ì´ëŠ” ëŠë‚Œì´ì—ìš”.             ê¸´ì¥ì„± ë‘í†µ        ì‹ ê²½ê³¼\n",
      "3   ë¨¸ë¦¿ì†ì´ ìš¸ë¦¬ëŠ” ë“¯ì´ ì•„íŒŒìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "4  ì•„ì¹¨ë§ˆë‹¤ ë¨¸ë¦¬ê°€ ë¬´ê²ê³  ëµí•´ìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "ì»¬ëŸ¼: ['colloquial', 'normalized_symptom', 'department']\n",
      "ë°ì´í„° ê°œìˆ˜: 435\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\medical_colloquial_sample.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())\n",
    "print(\"ì»¬ëŸ¼:\", df.columns.tolist())\n",
    "print(\"ë°ì´í„° ê°œìˆ˜:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c621320-dcc1-409f-9315-84b72f16c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ df ì»¬ëŸ¼: ['colloquial', 'normalized_symptom', 'department']\n",
      "ì¦ìƒ ì»¬ëŸ¼(symptom_col): normalized_symptom\n",
      "í…ìŠ¤íŠ¸ ì»¬ëŸ¼(text_col): colloquial\n",
      "\n",
      "ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê¹ƒ ì§„ë£Œê³¼: ['ì´ë¹„ì¸í›„ê³¼', 'ì •í˜•ì™¸ê³¼', 'í”¼ë¶€ê³¼', 'ì‹ ê²½ê³¼', 'ë‚´ê³¼', 'ì†Œí™”ê¸°ë‚´ê³¼']\n",
      "department\n",
      "ì´ë¹„ì¸í›„ê³¼    28\n",
      "ì •í˜•ì™¸ê³¼     50\n",
      "í”¼ë¶€ê³¼      41\n",
      "ì‹ ê²½ê³¼      94\n",
      "ë‚´ê³¼       32\n",
      "ì†Œí™”ê¸°ë‚´ê³¼    42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[ì´ë¹„ì¸í›„ê³¼] ì„ íƒëœ ì¦ìƒë“¤: ['ì´ëª…']\n",
      "\n",
      "[ì •í˜•ì™¸ê³¼] ì„ íƒëœ ì¦ìƒë“¤: []\n",
      "\n",
      "[í”¼ë¶€ê³¼] ì„ íƒëœ ì¦ìƒë“¤: ['ë‘í”¼ í†µì¦']\n",
      "\n",
      "[ì‹ ê²½ê³¼] ì„ íƒëœ ì¦ìƒë“¤: ['ë‘í†µ', 'ì–´ì§€ëŸ¼ì¦', 'ê¸´ì¥ì„± ë‘í†µ', 'í¸ë‘í†µ', 'ë‡Œ ì•ˆê°œ']\n",
      "\n",
      "[ë‚´ê³¼] ì„ íƒëœ ì¦ìƒë“¤: ['ì „ì‹  í”¼ë¡œ']\n",
      "\n",
      "[ì†Œí™”ê¸°ë‚´ê³¼] ì„ íƒëœ ì¦ìƒë“¤: ['ë³µí†µ', 'ì†Œí™”ë¶ˆëŸ‰', 'ìœ„ì—¼']\n",
      "\n",
      "UTìš© ìµœì¢… ì¦ìƒ ê°œìˆ˜: 11\n",
      "UTìš© ì¦ìƒ ëª©ë¡: ['ë‘í†µ', 'ì–´ì§€ëŸ¼ì¦', 'ë‡Œ ì•ˆê°œ', 'ë‘í”¼ í†µì¦', 'ë³µí†µ', 'ê¸´ì¥ì„± ë‘í†µ', 'í¸ë‘í†µ', 'ì „ì‹  í”¼ë¡œ', 'ìœ„ì—¼', 'ì´ëª…', 'ì†Œí™”ë¶ˆëŸ‰']\n",
      "\n",
      "UTìš© ë°ì´í„° ìƒ˜í”Œ ìˆ˜: 63\n",
      "ë¼ë²¨ ê°œìˆ˜: 11\n",
      "ë¼ë²¨ ë¶„í¬:\n",
      "label\n",
      "ë‘í†µ        24\n",
      "ì–´ì§€ëŸ¼ì¦       5\n",
      "ë³µí†µ         5\n",
      "ì†Œí™”ë¶ˆëŸ‰       5\n",
      "í¸ë‘í†µ        4\n",
      "ê¸´ì¥ì„± ë‘í†µ     4\n",
      "ë‡Œ ì•ˆê°œ       4\n",
      "ë‘í”¼ í†µì¦      3\n",
      "ì´ëª…         3\n",
      "ìœ„ì—¼         3\n",
      "ì „ì‹  í”¼ë¡œ      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "train / eval í¬ê¸°: 50 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "\n",
    "# í˜„ì¬ df ì»¬ëŸ¼ì— ë§ê²Œ ì¦ìƒ/í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„ ìë™ ì„¤ì •\n",
    "symptom_col = \"normalized_symptom\" if \"normalized_symptom\" in df.columns else \"label\"\n",
    "text_col    = \"colloquial\"         if \"colloquial\" in df.columns         else \"text\"\n",
    "\n",
    "print(\"í˜„ì¬ df ì»¬ëŸ¼:\", list(df.columns))\n",
    "print(\"ì¦ìƒ ì»¬ëŸ¼(symptom_col):\", symptom_col)\n",
    "print(\"í…ìŠ¤íŠ¸ ì»¬ëŸ¼(text_col):\", text_col)\n",
    "\n",
    "# 1) ìš°ë¦¬ê°€ UTì—ì„œ ë‹¤ë£¨ê³  ì‹¶ì€ ì§„ë£Œê³¼ í›„ë³´\n",
    "target_depts = [\n",
    "    \"ì´ë¹„ì¸í›„ê³¼\",\n",
    "    \"ì •í˜•ì™¸ê³¼\",\n",
    "    \"í”¼ë¶€ê³¼\",\n",
    "    \"ì‹ ê²½ê³¼\",\n",
    "    \"ë‚´ê³¼\",\n",
    "    \"ì†Œí™”ê¸°ë‚´ê³¼\",\n",
    "]\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì§„ë£Œê³¼ë§Œ ì‚¬ìš©\n",
    "dept_counts = df[\"department\"].value_counts()\n",
    "valid_target_depts = [d for d in target_depts if d in dept_counts.index]\n",
    "\n",
    "print(\"\\nì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê¹ƒ ì§„ë£Œê³¼:\", valid_target_depts)\n",
    "print(dept_counts[valid_target_depts])\n",
    "\n",
    "# 2) ê° ì§„ë£Œê³¼ë§ˆë‹¤ ìì£¼ ë‚˜ì˜¤ëŠ” ì¦ìƒ ìƒìœ„ Kê°œì”© ë½‘ê¸°\n",
    "MIN_SYMPTOM_COUNT = 3   # ìµœì†Œ 3ê°œ ì´ìƒ ìˆëŠ” ì¦ìƒë§Œ\n",
    "TOP_K_PER_DEPT    = 5   # ì§„ë£Œê³¼ë‹¹ ìµœëŒ€ 5ê°œ ì¦ìƒ\n",
    "\n",
    "ut_labels = set()\n",
    "\n",
    "for dept in valid_target_depts:\n",
    "    sub = df[df[\"department\"] == dept]\n",
    "    vc = sub[symptom_col].value_counts()\n",
    "\n",
    "    chosen = vc[vc >= MIN_SYMPTOM_COUNT].head(TOP_K_PER_DEPT).index\n",
    "    print(f\"\\n[{dept}] ì„ íƒëœ ì¦ìƒë“¤:\", list(chosen))\n",
    "    ut_labels.update(chosen)\n",
    "\n",
    "ut_labels = list(ut_labels)\n",
    "print(\"\\nUTìš© ìµœì¢… ì¦ìƒ ê°œìˆ˜:\", len(ut_labels))\n",
    "print(\"UTìš© ì¦ìƒ ëª©ë¡:\", ut_labels)\n",
    "\n",
    "# 3) ì´ ì¦ìƒë“¤ë§Œ ëª¨ì€ UTìš© ë°ì´í„°í”„ë ˆì„\n",
    "df_ut = df[df[symptom_col].isin(ut_labels)].copy()\n",
    "print(\"\\nUTìš© ë°ì´í„° ìƒ˜í”Œ ìˆ˜:\", len(df_ut))\n",
    "\n",
    "# 4) ì»¬ëŸ¼ ì´ë¦„ì„ í•™ìŠµìš©ìœ¼ë¡œ í†µì¼: text / label / department\n",
    "df_ut = df_ut.rename(\n",
    "    columns={\n",
    "        text_col: \"text\",\n",
    "        symptom_col: \"label\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5) ë¼ë²¨ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "df_ut[\"label_id\"] = le.fit_transform(df_ut[\"label\"])\n",
    "\n",
    "print(\"ë¼ë²¨ ê°œìˆ˜:\", df_ut[\"label_id\"].nunique())\n",
    "print(\"ë¼ë²¨ ë¶„í¬:\")\n",
    "print(df_ut[\"label\"].value_counts())\n",
    "\n",
    "# 6) train / eval ë¶„ë¦¬\n",
    "train_df, eval_df = train_test_split(\n",
    "    df_ut,\n",
    "    test_size=0.2,\n",
    "    stratify=df_ut[\"label_id\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_ds  = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "print(\"\\ntrain / eval í¬ê¸°:\", len(train_ds), len(eval_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f18851-f757-4755-b459-680f109f5587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì›ë˜ df_ut ë¼ë²¨ ë¶„í¬ ===\n",
      "label\n",
      "ë‘í†µ        24\n",
      "ì–´ì§€ëŸ¼ì¦       5\n",
      "ë³µí†µ         5\n",
      "ì†Œí™”ë¶ˆëŸ‰       5\n",
      "í¸ë‘í†µ        4\n",
      "ê¸´ì¥ì„± ë‘í†µ     4\n",
      "ë‡Œ ì•ˆê°œ       4\n",
      "ë‘í”¼ í†µì¦      3\n",
      "ì´ëª…         3\n",
      "ìœ„ì—¼         3\n",
      "ì „ì‹  í”¼ë¡œ      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== ì˜¤ë²„ìƒ˜í”Œë§ í›„ df_ut ë¼ë²¨ ë¶„í¬ ===\n",
      "label\n",
      "ë‘í†µ        24\n",
      "ì´ëª…        24\n",
      "ë‘í”¼ í†µì¦     24\n",
      "í¸ë‘í†µ       24\n",
      "ë³µí†µ        24\n",
      "ìœ„ì—¼        24\n",
      "ê¸´ì¥ì„± ë‘í†µ    24\n",
      "ì „ì‹  í”¼ë¡œ     24\n",
      "ë‡Œ ì•ˆê°œ      24\n",
      "ì†Œí™”ë¶ˆëŸ‰      24\n",
      "ì–´ì§€ëŸ¼ì¦      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"=== ì›ë˜ df_ut ë¼ë²¨ ë¶„í¬ ===\")\n",
    "print(df_ut[\"label\"].value_counts())\n",
    "\n",
    "# 1) ê° ë¼ë²¨ì´ ëª‡ ê°œì”© ìˆëŠ”ì§€ í™•ì¸\n",
    "label_counts = df_ut[\"label\"].value_counts()\n",
    "max_count = label_counts.max()   # ê°€ì¥ ë§ì€ ë¼ë²¨ ê°œìˆ˜ë§Œí¼ ë§ì¶°ì¤„ ê±°ì•¼\n",
    "\n",
    "balanced_list = []\n",
    "\n",
    "for label, group in df_ut.groupby(\"label\"):\n",
    "    # ê° ë¼ë²¨ì„ max_count ê°œìˆ˜ë§Œí¼ ë³µì›ì¶”ì¶œ(ì¤‘ë³µ í—ˆìš©)ë¡œ ìƒ˜í”Œë§\n",
    "    balanced = group.sample(n=max_count, replace=True, random_state=42)\n",
    "    balanced_list.append(balanced)\n",
    "\n",
    "df_ut_balanced = (\n",
    "    pd.concat(balanced_list)\n",
    "      .sample(frac=1, random_state=42)  # ì„ê¸°\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_ut = df_ut_balanced.copy()\n",
    "\n",
    "print(\"\\n=== ì˜¤ë²„ìƒ˜í”Œë§ í›„ df_ut ë¼ë²¨ ë¶„í¬ ===\")\n",
    "print(df_ut[\"label\"].value_counts())\n",
    "\n",
    "# ë¼ë²¨ ì¸ì½”ë”© ë‹¤ì‹œ\n",
    "le = LabelEncoder()\n",
    "df_ut[\"label_id\"] = le.fit_transform(df_ut[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076d36f6-f451-46f8-af04-8f80bdbdad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1324a4a7d64b22bc41ceffa83cb39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd829c0237a641d5bdc89b944ff91575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  605,\n",
       "  9,\n",
       "  684,\n",
       "  21,\n",
       "  353,\n",
       "  16,\n",
       "  169,\n",
       "  221,\n",
       "  239,\n",
       "  5,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6ë²ˆ ì…€: í† í¬ë‚˜ì´ì§•\n",
    "def tokenize_batch(batch):\n",
    "    encoded = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )\n",
    "    encoded[\"labels\"] = batch[\"label_id\"]\n",
    "    return encoded\n",
    "\n",
    "train_ds_tok = train_ds.map(tokenize_batch, batched=True)\n",
    "eval_ds_tok  = eval_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Trainerì— í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì œê±°\n",
    "cols_keep = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "cols_drop = [c for c in train_ds_tok.column_names if c not in cols_keep]\n",
    "\n",
    "train_ds_tok = train_ds_tok.remove_columns(cols_drop)\n",
    "eval_ds_tok  = eval_ds_tok.remove_columns(cols_drop)\n",
    "\n",
    "train_ds_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b471521-5d4f-4430-983e-d51bd1577505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jang8\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at madatnlp/km-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jang8\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.025290</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.061728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.959257</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.061728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.902410</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.061728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.849226</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.061728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.815848</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.061728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=1.972869655064174, metrics={'train_runtime': 60.6925, 'train_samples_per_second': 4.119, 'train_steps_per_second': 0.577, 'total_flos': 8222884896000.0, 'train_loss': 1.972869655064174, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "# num_labels = df_filtered[\"label_id\"].nunique()   # (X)\n",
    "num_labels = df_ut[\"label_id\"].nunique()           # (O)\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"madatnlp/km-bert\",\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kmbert_mednorm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_cls,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_tok,\n",
    "    eval_dataset=eval_ds_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7daf2fd3-40be-4bad-9833-13fd82247c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n",
      "[í•œìª½ ë¨¸ë¦¬ê°€ ì§€ëˆì§€ëˆ ì‘¤ì…”ìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n",
      "[ì†ì´ ë©”ìŠ¤ê»ê³  í† í•  ê²ƒ ê°™ì•„ìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n",
      "[í—ˆë¦¬ê°€ ë»ê·¼í•˜ê³  ìš±ì‹ ê±°ë ¤ìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict_symptom(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cls(**inputs).logits\n",
    "\n",
    "    pred_ids = logits.argmax(dim=-1).tolist()\n",
    "    pred_labels = le.inverse_transform(pred_ids)\n",
    "\n",
    "    for t, lab in zip(texts, pred_labels):\n",
    "        print(f\"[{t}] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: {lab}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ëª‡ ê°œ í•´ë³´ê¸°\n",
    "test_texts = [\n",
    "    \"ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”\",\n",
    "    \"í•œìª½ ë¨¸ë¦¬ê°€ ì§€ëˆì§€ëˆ ì‘¤ì…”ìš”\",\n",
    "    \"ì†ì´ ë©”ìŠ¤ê»ê³  í† í•  ê²ƒ ê°™ì•„ìš”\",\n",
    "    \"í—ˆë¦¬ê°€ ë»ê·¼í•˜ê³  ìš±ì‹ ê±°ë ¤ìš”\",\n",
    "]\n",
    "\n",
    "predict_symptom(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b302c5c-531b-4e1a-b119-11d986fdbb02",
   "metadata": {},
   "source": [
    "âœ” 1) KM-BERT ëª¨ë¸ ë¡œë“œ\n",
    "âœ” 2) í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "âœ” 3) label encoder ì¤€ë¹„\n",
    "âœ” 4) predict_symptom(texts) í•¨ìˆ˜ê¹Œì§€ ì™„ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a47aae-c043-4791-9d04-163c65d4b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¦ìƒâ†’ì§„ë£Œê³¼ ë§¤í•‘ ì˜ˆì‹œ 5ê°œ:\n",
      "ê¸´ì¥ì„± ë‘í†µ â†’ ì‹ ê²½ê³¼\n",
      "ë‡Œ ì•ˆê°œ â†’ ì‹ ê²½ê³¼\n",
      "ë‘í†µ â†’ ì‹ ê²½ê³¼\n",
      "ë‘í”¼ í†µì¦ â†’ í”¼ë¶€ê³¼\n",
      "ë³µí†µ â†’ ì†Œí™”ê¸°ë‚´ê³¼\n"
     ]
    }
   ],
   "source": [
    "# df_ut ì•ˆì— text / label / label_id / department ê°€ ìˆë‹¤ê³  ê°€ì •\n",
    "\n",
    "# 1) ì¦ìƒ(label)ë³„ë¡œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ì§„ë£Œê³¼ë¥¼ ëŒ€í‘œê°’ìœ¼ë¡œ ì‚¬ìš©\n",
    "symptom_to_dept = (\n",
    "    df_ut.groupby(\"label\")[\"department\"]\n",
    "         .agg(lambda x: x.value_counts().idxmax())\n",
    "         .to_dict()\n",
    ")\n",
    "\n",
    "print(\"ì¦ìƒâ†’ì§„ë£Œê³¼ ë§¤í•‘ ì˜ˆì‹œ 5ê°œ:\")\n",
    "for k in list(symptom_to_dept.keys())[:5]:\n",
    "    print(k, \"â†’\", symptom_to_dept[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a400dcee-a3df-4417-9961-0419e2edf974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ì…ë ¥ë¬¸ì¥': 'ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”', 'ì •ê·œí™”_ì¦ìƒ': 'ë‘í†µ', 'ì¶”ì²œ_ì§„ë£Œê³¼': 'ì‹ ê²½ê³¼', 'ì„¤ëª…': \"ì…ë ¥í•˜ì‹  í‘œí˜„ì€ 'ë‘í†µ' ë²”ì£¼ë¡œ ì •ê·œí™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì¦ìƒì€ ë³´í†µ 'ì‹ ê²½ê³¼'ì—ì„œ ì£¼ë¡œ ë‹¤ë£¨ëŠ” ì¦ìƒì…ë‹ˆë‹¤.\", 'ì¶”ê°€ì§ˆë¬¸': \"ì´ 'ë‘í†µ' ì¦ìƒì€ ì–¸ì œë¶€í„°, ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict_symptom_single(text: str) -> str:\n",
    "    \"\"\"í•œ ë¬¸ì¥ì— ëŒ€í•´ ì •ê·œí™”ëœ ì¦ìƒ ë¼ë²¨ë§Œ ë°˜í™˜\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cls(**inputs).logits\n",
    "\n",
    "    pred_id = logits.argmax(dim=-1).item()\n",
    "    pred_label = le.inverse_transform([pred_id])[0]\n",
    "    return pred_label\n",
    "\n",
    "\n",
    "def mediq_reply(user_text: str, body_part: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    êµ¬ì–´ì²´ ì¦ìƒì„ ì…ë ¥í•˜ë©´:\n",
    "    - ì •ê·œí™”ëœ ì¦ìƒ\n",
    "    - CSV ê¸°ë°˜ ì§„ë£Œê³¼\n",
    "    - ê°„ë‹¨í•œ ì„¤ëª…(í…œí”Œë¦¿)\n",
    "    - í›„ì† ì§ˆë¬¸\n",
    "    ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    symptom = predict_symptom_single(user_text)\n",
    "\n",
    "    # CSVì—ì„œ ë½‘ì€ ë§¤í•‘ë§Œ ì‚¬ìš©\n",
    "    dept = symptom_to_dept.get(symptom, \"ì§„ë£Œê³¼ ì •ë³´ ì—†ìŒ\")\n",
    "\n",
    "    # ì„¤ëª…ê³¼ ì§ˆë¬¸ì€ 'ì˜ë¯¸ë¥¼ ê¾¸ë©°ì£¼ëŠ” ë¬¸ì¥ í…œí”Œë¦¿' ìˆ˜ì¤€ìœ¼ë¡œë§Œ\n",
    "    desc = f\"ì…ë ¥í•˜ì‹  í‘œí˜„ì€ '{symptom}' ë²”ì£¼ë¡œ ì •ê·œí™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì¦ìƒì€ ë³´í†µ '{dept}'ì—ì„œ ì£¼ë¡œ ë‹¤ë£¨ëŠ” ì¦ìƒì…ë‹ˆë‹¤.\"\n",
    "    if body_part:\n",
    "        desc += f\" ì„ íƒí•˜ì‹  ë¶€ìœ„ëŠ” '{body_part}'ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    follow_up = f\"ì´ '{symptom}' ì¦ìƒì€ ì–¸ì œë¶€í„°, ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?\"\n",
    "\n",
    "    return {\n",
    "        \"ì…ë ¥ë¬¸ì¥\": user_text,\n",
    "        \"ì •ê·œí™”_ì¦ìƒ\": symptom,\n",
    "        \"ì¶”ì²œ_ì§„ë£Œê³¼\": dept,\n",
    "        \"ì„¤ëª…\": desc,\n",
    "        \"ì¶”ê°€ì§ˆë¬¸\": follow_up,\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(mediq_reply(\"ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a6c1f5-07d5-45f6-b204-db86ed6b9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë°°ê°€ ì•„íŒŒìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n",
      "[í† í•  ê²ƒ ê°™ì•„ìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n",
      "[ì†ì´ ì“°ë ¤ìš”] -> ì˜ˆì¸¡ ì •ê·œí™” ì¦ìƒ: ë‘í†µ\n"
     ]
    }
   ],
   "source": [
    "predict_symptom([\"ë°°ê°€ ì•„íŒŒìš”\", \"í† í•  ê²ƒ ê°™ì•„ìš”\", \"ì†ì´ ì“°ë ¤ìš”\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a8d364-7cdb-4c48-8ee3-e8183a36d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== df_ut ìƒ˜í”Œ 5ê°œ ===\n",
      "                      text  label department  label_id\n",
      "0        ê³ ê°œë¥¼ ìˆ™ì´ë©´ ë¨¸ë¦¬ê°€ ì°Œë¦¿í•´ìš”.     ë‘í†µ        ì‹ ê²½ê³¼         2\n",
      "1     ê·€ì—ì„œ ë¬¼ ì†Œë¦¬ ê°™ì€ ê²Œ ê³„ì† ë‚˜ìš”.     ì´ëª…      ì´ë¹„ì¸í›„ê³¼         8\n",
      "2        ë¨¸ë¦¬ë¥¼ ê°ì„ ë•Œ ë‘í”¼ê°€ ì•„íŒŒìš”.  ë‘í”¼ í†µì¦        í”¼ë¶€ê³¼         3\n",
      "3         í•œìª½ ë¨¸ë¦¬ê°€ ì§€ëˆì§€ëˆ ì‘¤ì…”ìš”.    í¸ë‘í†µ        ì‹ ê²½ê³¼        10\n",
      "4  ë°°ì—ì„œ ì°Œë¦¿í•˜ê²Œ ì „ê¸°ê°€ íë¥´ëŠ” ë“¯ ì•„íŒŒìš”.     ë³µí†µ      ì†Œí™”ê¸°ë‚´ê³¼         4\n",
      "\n",
      "=== df_ut ë¼ë²¨ ë¶„í¬ ===\n",
      "label\n",
      "ë‘í†µ        24\n",
      "ì´ëª…        24\n",
      "ë‘í”¼ í†µì¦     24\n",
      "í¸ë‘í†µ       24\n",
      "ë³µí†µ        24\n",
      "ìœ„ì—¼        24\n",
      "ê¸´ì¥ì„± ë‘í†µ    24\n",
      "ì „ì‹  í”¼ë¡œ     24\n",
      "ë‡Œ ì•ˆê°œ      24\n",
      "ì†Œí™”ë¶ˆëŸ‰      24\n",
      "ì–´ì§€ëŸ¼ì¦      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=== df_ut ìƒ˜í”Œ 5ê°œ ===\")\n",
    "print(df_ut.head())\n",
    "\n",
    "print(\"\\n=== df_ut ë¼ë²¨ ë¶„í¬ ===\")\n",
    "print(df_ut[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3138d4ff-3324-45b8-aacb-cb993b960bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON í´ë” ìŠ¤ìº” ì‹œì‘ ===\n",
      "\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\2.ë‹µë³€\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\\TL\\1.ì§ˆë¬¸\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\\TL\\2.ë‹µë³€\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\01.ì›ì²œë°ì´í„°\\VS\\1.ì§ˆë¬¸\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\01.ì›ì²œë°ì´í„°\\VS\\2.ë‹µë³€\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\02.ë¼ë²¨ë§ë°ì´í„°\\VL\\1.ì§ˆë¬¸\n",
      "ğŸ“‚ ìŠ¤ìº” ì¤‘: C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\02.ë¼ë²¨ë§ë°ì´í„°\\VL\\2.ë‹µë³€\n",
      "\n",
      "=== ğŸ“„ JSON ìˆ˜ì§‘ ì™„ë£Œ ===\n",
      "ì´ JSON íŒŒì¼ ê°œìˆ˜: 6993458\n",
      "\n",
      "ğŸ“Œ ì˜ˆì‹œ 10ê°œ:\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160428.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160429.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160430.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160431.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160432.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160433.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160434.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160435.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160436.json\n",
      " - C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\\3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\\ê°ì—¼ì„±ì§ˆí™˜\\HIV ê°ì—¼\\ê²€ì§„\\HC-Q-1160437.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Œ ìµœìƒìœ„ í´ë” (ì—¬ê¸°ê¹Œì§€ê°€ '120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°')\n",
    "base_dir = r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI ì‚¬ì „í•™ìŠµìš© í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°\"\n",
    "\n",
    "# ğŸ“Œ ë„ˆê°€ ìš”ì²­í•œ ê²½ë¡œ êµ¬ì¡° ë°˜ì˜í•œ ë²„ì „\n",
    "json_dirs = [\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\1.ì§ˆë¬¸\",\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\01.ì›ì²œë°ì´í„°\\TS\\2.ë‹µë³€\",\n",
    "\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\\TL\\1.ì§ˆë¬¸\",\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\\TL\\2.ë‹µë³€\",\n",
    "\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\01.ì›ì²œë°ì´í„°\\VS\\1.ì§ˆë¬¸\",\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\01.ì›ì²œë°ì´í„°\\VS\\2.ë‹µë³€\",\n",
    "\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\02.ë¼ë²¨ë§ë°ì´í„°\\VL\\1.ì§ˆë¬¸\",\n",
    "    r\"3.ê°œë°©ë°ì´í„°\\1.ë°ì´í„°\\Validation\\02.ë¼ë²¨ë§ë°ì´í„°\\VL\\2.ë‹µë³€\",\n",
    "]\n",
    "\n",
    "json_paths = []\n",
    "\n",
    "print(\"=== JSON í´ë” ìŠ¤ìº” ì‹œì‘ ===\\n\")\n",
    "\n",
    "for rel_path in json_dirs:\n",
    "    target = os.path.join(base_dir, rel_path)\n",
    "\n",
    "    if not os.path.exists(target):\n",
    "        print(f\"âš  ê²½ë¡œ ì—†ìŒ â†’ {target}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ğŸ“‚ ìŠ¤ìº” ì¤‘: {target}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(target):\n",
    "        for fname in files:\n",
    "            if fname.lower().endswith(\".json\"):\n",
    "                json_paths.append(os.path.join(root, fname))\n",
    "\n",
    "print(\"\\n=== ğŸ“„ JSON ìˆ˜ì§‘ ì™„ë£Œ ===\")\n",
    "print(\"ì´ JSON íŒŒì¼ ê°œìˆ˜:\", len(json_paths))\n",
    "print(\"\\nğŸ“Œ ì˜ˆì‹œ 10ê°œ:\")\n",
    "for p in json_paths[:10]:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae545911-b190-425d-8059-557f88b2295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colloquial</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>disease</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HIV ê°ì—¼ì„ ì˜ì‹¬í•˜ê³  ìˆëŠ”ë°, ì–´ë–¤ ê³³ì—ì„œ ê²€ì§„ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>HIV ê°ì—¼</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì„± ì ‘ì´‰ ì´í›„ HIV ê°ì—¼ìœ¼ë¡œ ì¸í•œ ì¦ìƒì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œê³  ì‹¶ì–´ìš”.</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>HIV ê°ì—¼</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HIV ê°ì—¼ì„ ì§„ë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ê²€ì‚¬ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>HIV ê°ì—¼</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HIV ê°ì—¼ì„ ì˜ì‹¬í•  ë•Œ ì–´ë–¤ ê²€ì‚¬ë¥¼ í†µí•´ ì§„ë‹¨ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>HIV ê°ì—¼</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HIV ê°ì—¼ ê²€ì‚¬ëŠ” ë°±í˜ˆêµ¬ ìˆ˜ì¹˜ ê°ì†Œë¡œ ì¸í•´ ê¶Œì¥ë˜ëŠ” ê²€ì‚¬ì¸ê°€ìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>HIV ê°ì—¼</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  colloquial answer category disease  \\\n",
       "0      HIV ê°ì—¼ì„ ì˜ì‹¬í•˜ê³  ìˆëŠ”ë°, ì–´ë–¤ ê³³ì—ì„œ ê²€ì§„ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?           ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
       "1  ì„± ì ‘ì´‰ ì´í›„ HIV ê°ì—¼ìœ¼ë¡œ ì¸í•œ ì¦ìƒì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œê³  ì‹¶ì–´ìš”.           ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
       "2      HIV ê°ì—¼ì„ ì§„ë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ê²€ì‚¬ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?           ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
       "3       HIV ê°ì—¼ì„ ì˜ì‹¬í•  ë•Œ ì–´ë–¤ ê²€ì‚¬ë¥¼ í†µí•´ ì§„ë‹¨ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?           ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
       "4       HIV ê°ì—¼ ê²€ì‚¬ëŠ” ë°±í˜ˆêµ¬ ìˆ˜ì¹˜ ê°ì†Œë¡œ ì¸í•´ ê¶Œì¥ë˜ëŠ” ê²€ì‚¬ì¸ê°€ìš”?           ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
       "\n",
       "                                              source  \n",
       "0  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "1  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "2  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "3  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "4  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths = json_paths[:1000]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for path in test_paths:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        question = data.get(\"question\", \"\")\n",
    "        answer = data.get(\"answer\", \"\")\n",
    "        disease_category = data.get(\"disease_category\", \"\")\n",
    "        disease_name = data.get(\"disease_name\", {}).get(\"kor\", \"\")\n",
    "        \n",
    "        rows.append({\n",
    "            \"colloquial\": question,\n",
    "            \"answer\": answer,\n",
    "            \"category\": disease_category,\n",
    "            \"disease\": disease_name,\n",
    "            \"source\": path\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_test = pd.DataFrame(rows)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f08c116-bdcf-4c12-9a82-39a88ba1372b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colloquial</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>disease</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë¦¼í”„ì¢… ì¹˜ë£Œì—ëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ìš´ë™ ì¹˜ë£Œê°€ íš¨ê³¼ì ì¸ê°€ìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ì¢…ì–‘í˜ˆì•¡ì§ˆí™˜</td>\n",
       "      <td>ë¦¼í”„ì¢…</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì†ëª© ìˆ˜ê·¼ê´€ ì¦í›„êµ°ì˜ ì¦ìƒì„ ìì„¸íˆ ì•Œê³  ì‹¶ì–´ìš”.</td>\n",
       "      <td></td>\n",
       "      <td>ê·¼ê³¨ê²©ì§ˆí™˜</td>\n",
       "      <td>ì†ëª© ìˆ˜ê·¼ê´€ ì¦í›„êµ°</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>íê²°í•µ ì˜ˆë°©ì„ ìœ„í•´ ì–´ë–¤ ì‹ì´ ê´€ë ¨ ìŠµê´€ì„ ì§€ì¼œì•¼ í• ê¹Œìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ê°ì—¼ì„±ì§ˆí™˜</td>\n",
       "      <td>ê²°í•µ</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë‘í†µê³¼ ê°™ì€ ì§ˆí™˜ì„ ì˜ˆë°©í•˜ëŠ” ë°©ë²•ì´ ìˆì„ê¹Œìš”?</td>\n",
       "      <td></td>\n",
       "      <td>ì‘ê¸‰ì§ˆí™˜</td>\n",
       "      <td>ë‘í†µ</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì‚°í›„ íšŒë³µì„ ìœ„í•œ ì‹ì´ì™€ ìƒí™œ ìŠµê´€ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.</td>\n",
       "      <td></td>\n",
       "      <td>ì—¬ì„±ì§ˆí™˜</td>\n",
       "      <td>ìœ ì‚°</td>\n",
       "      <td>C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         colloquial answer category     disease  \\\n",
       "0    ë¦¼í”„ì¢… ì¹˜ë£Œì—ëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ìš´ë™ ì¹˜ë£Œê°€ íš¨ê³¼ì ì¸ê°€ìš”?          ì¢…ì–‘í˜ˆì•¡ì§ˆí™˜         ë¦¼í”„ì¢…   \n",
       "1       ì†ëª© ìˆ˜ê·¼ê´€ ì¦í›„êµ°ì˜ ì¦ìƒì„ ìì„¸íˆ ì•Œê³  ì‹¶ì–´ìš”.           ê·¼ê³¨ê²©ì§ˆí™˜  ì†ëª© ìˆ˜ê·¼ê´€ ì¦í›„êµ°   \n",
       "2  íê²°í•µ ì˜ˆë°©ì„ ìœ„í•´ ì–´ë–¤ ì‹ì´ ê´€ë ¨ ìŠµê´€ì„ ì§€ì¼œì•¼ í• ê¹Œìš”?           ê°ì—¼ì„±ì§ˆí™˜          ê²°í•µ   \n",
       "3         ë‘í†µê³¼ ê°™ì€ ì§ˆí™˜ì„ ì˜ˆë°©í•˜ëŠ” ë°©ë²•ì´ ìˆì„ê¹Œìš”?            ì‘ê¸‰ì§ˆí™˜          ë‘í†µ   \n",
       "4    ì‚°í›„ íšŒë³µì„ ìœ„í•œ ì‹ì´ì™€ ìƒí™œ ìŠµê´€ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.            ì—¬ì„±ì§ˆí™˜          ìœ ì‚°   \n",
       "\n",
       "                                              source  \n",
       "0  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "1  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "2  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "3  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  \n",
       "4  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ì „ì²´ json_paths ì¤‘ì—ì„œ 1000ê°œ ëœë¤ ì„ íƒ\n",
    "test_paths = random.sample(json_paths, min(1000, len(json_paths)))\n",
    "\n",
    "rows = []\n",
    "\n",
    "for path in test_paths:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        question = data.get(\"question\", \"\")\n",
    "        answer = data.get(\"answer\", \"\")\n",
    "        disease_category = data.get(\"disease_category\", \"\")\n",
    "\n",
    "        # disease_name dict(eng/kor) ë˜ëŠ” ë¬¸ìì—´ ì¼€ì´ìŠ¤ ìë™ì²˜ë¦¬\n",
    "        raw_dis = data.get(\"disease_name\", \"\")\n",
    "        if isinstance(raw_dis, dict):\n",
    "            disease_name = raw_dis.get(\"kor\", \"\")\n",
    "        else:\n",
    "            disease_name = raw_dis\n",
    "\n",
    "        rows.append({\n",
    "            \"colloquial\": question.strip(),\n",
    "            \"answer\": answer.strip(),\n",
    "            \"category\": disease_category.strip(),\n",
    "            \"disease\": disease_name.strip(),\n",
    "            \"source\": path\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_test = pd.DataFrame(rows)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbc5bb5-8fd5-4400-a3db-51f7903c4e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362, 5)\n",
      "disease\n",
      "ê³ í˜ˆì••                 7\n",
      "íŒ¨í˜ˆì¦                 4\n",
      "ê±°ë¶ëª© ì¦í›„êµ°             4\n",
      "ì•ˆê²€í•˜ìˆ˜                3\n",
      "ë°œìœ¡ë¶€ì „ì„± ì¢Œì‹¬ ì¦í›„êµ°        3\n",
      "ì†Œì¥ì•”                 3\n",
      "êµ¬ìˆœì—´                 3\n",
      "ë‡Œí•˜ìˆ˜ì²´ ì–‘ì„± ë° ì•…ì„± ì‹ ìƒë¬¼    3\n",
      "í—¤ë¥´í˜ìŠ¤ ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼        3\n",
      "ì†ëª© ìˆ˜ê·¼ê´€ ì¦í›„êµ°          3\n",
      "ìê¶ê²½ë¶€ì•”               3\n",
      "ë‡Œê²½ìƒ‰                 3\n",
      "ì²™ì¶”ì¸¡ë§Œì¦               3\n",
      "ë©´ì—­í˜ˆì†ŒíŒê°ì†Œì¦            3\n",
      "ì¹˜ë§¤                  3\n",
      "ì²œì‹                  3\n",
      "ê³¨ë‹¤ê³µì¦                3\n",
      "í”¼ë¶€ì•”                 3\n",
      "ê°„ì—¼                  3\n",
      "ë‘í†µ                  3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test[\"disease\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5c3ea3e-406c-4c67-b8bb-25a0b6c3f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ JSON ê°œìˆ˜: 6993458\n",
      "=== ì „ì²´ íŒŒì‹± ì‹œì‘ ===\n",
      "\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_1.csv (100000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_2.csv (200000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_3.csv (300000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_4.csv (400000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_5.csv (500000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_6.csv (600000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_7.csv (700000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_8.csv (800000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_9.csv (900000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_10.csv (1000000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_11.csv (1100000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_12.csv (1200000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_13.csv (1300000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_14.csv (1400000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_15.csv (1500000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_16.csv (1600000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_17.csv (1700000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_18.csv (1800000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_19.csv (1900000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_20.csv (2000000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_21.csv (2100000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_22.csv (2200000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_23.csv (2300000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_24.csv (2400000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ì €ì¥ ì™„ë£Œ: all_data_part_25.csv (2500000/6993458 ì™„ë£Œ)\n",
      "ğŸ’¾ ë§ˆì§€ë§‰ ì €ì¥ ì™„ë£Œ: all_data_part_26.csv\n",
      "\n",
      "=== ì „ì²´ íŒŒì‹± ì™„ë£Œ! ===\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "CHUNK_SIZE = 100000   # 10ë§Œê°œì”© CSV ì €ì¥\n",
    "rows = []\n",
    "file_index = 1\n",
    "processed = 0\n",
    "\n",
    "print(f\"ì´ JSON ê°œìˆ˜: {len(json_paths)}\")\n",
    "print(\"=== ì „ì²´ íŒŒì‹± ì‹œì‘ ===\\n\")\n",
    "\n",
    "for idx, path in enumerate(json_paths):\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        question = data.get(\"question\", \"\")\n",
    "        answer = data.get(\"answer\", \"\")\n",
    "        disease_category = data.get(\"disease_category\", \"\")\n",
    "\n",
    "        # disease_nameì´ dictì¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì²˜ë¦¬\n",
    "        raw_dis = data.get(\"disease_name\", \"\")\n",
    "        if isinstance(raw_dis, dict):\n",
    "            disease_name = raw_dis.get(\"kor\", \"\")\n",
    "        else:\n",
    "            disease_name = raw_dis\n",
    "\n",
    "        rows.append({\n",
    "            \"colloquial\": question.strip(),\n",
    "            \"answer\": answer.strip(),\n",
    "            \"category\": disease_category.strip(),\n",
    "            \"disease\": disease_name.strip(),\n",
    "            \"source\": path\n",
    "        })\n",
    "\n",
    "    except:\n",
    "        # ê¹¨ì§„ json íŒŒì¼ì€ skip\n",
    "        continue\n",
    "\n",
    "    processed += 1\n",
    "\n",
    "    # 10ë§Œ ê°œ ëª¨ì˜€ì„ ë•Œ CSVë¡œ ì €ì¥\n",
    "    if processed % CHUNK_SIZE == 0:\n",
    "        df_chunk = pd.DataFrame(rows)\n",
    "        save_path = f\"all_data_part_{file_index}.csv\"\n",
    "        df_chunk.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path} ({processed}/{len(json_paths)} ì™„ë£Œ)\")\n",
    "        \n",
    "        rows = []  # ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "        file_index += 1\n",
    "\n",
    "# ë§ˆì§€ë§‰ì— ë‚¨ì€ ë°ì´í„° ì €ì¥\n",
    "if len(rows) > 0:\n",
    "    df_chunk = pd.DataFrame(rows)\n",
    "    save_path = f\"all_data_part_{file_index}.csv\"\n",
    "    df_chunk.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ğŸ’¾ ë§ˆì§€ë§‰ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "print(\"\\n=== ì „ì²´ íŒŒì‹± ì™„ë£Œ! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cf966-80e4-48fe-826e-95972607ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_paths))  # 6993458 ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f73055d-ad49-4d24-994c-f16e7e387e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³‘í•© ì¤‘ â†’ all_data_part_1.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_10.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_11.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_12.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_13.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_14.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_15.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_16.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_17.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_18.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_19.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_2.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_20.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_21.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_22.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_23.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_24.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_25.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_26.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_3.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_4.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_5.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_6.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_7.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_8.csv\n",
      "ë³‘í•© ì¤‘ â†’ all_data_part_9.csv\n",
      "ë³‘í•© ì¤‘ â†’ health_colloquial_raw.csv\n",
      "ë³‘í•© ì¤‘ â†’ medical_colloquial_sample.csv\n",
      "\n",
      "ğŸ‰ ë³‘í•© ì™„ë£Œ!\n",
      "ì´ ë°ì´í„° ìˆ˜: 2625131\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# CSVë“¤ì´ ìˆëŠ” í´ë”\n",
    "data_dir = r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\"\n",
    "\n",
    "# ëª¨ë“  CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    path = os.path.join(data_dir, f)\n",
    "    print(\"ë³‘í•© ì¤‘ â†’\", f)\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# ìµœì¢… í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ì €ì¥\n",
    "final_path = os.path.join(data_dir, \"merged_all_data.csv\")\n",
    "final_df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nğŸ‰ ë³‘í•© ì™„ë£Œ!\")\n",
    "print(\"ì´ ë°ì´í„° ìˆ˜:\", len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45226bf-e015-4801-9fa7-cbaea925ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jang8\\AppData\\Local\\Temp\\ipykernel_15056\\3302758387.py:3: DtypeWarning: Columns (2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\merged_all_data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  colloquial  answer category disease  \\\n",
      "0      HIV ê°ì—¼ì„ ì˜ì‹¬í•˜ê³  ìˆëŠ”ë°, ì–´ë–¤ ê³³ì—ì„œ ê²€ì§„ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?     NaN    ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
      "1  ì„± ì ‘ì´‰ ì´í›„ HIV ê°ì—¼ìœ¼ë¡œ ì¸í•œ ì¦ìƒì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì•Œê³  ì‹¶ì–´ìš”.     NaN    ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
      "2      HIV ê°ì—¼ì„ ì§„ë‹¨í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ê²€ì‚¬ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?     NaN    ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
      "3       HIV ê°ì—¼ì„ ì˜ì‹¬í•  ë•Œ ì–´ë–¤ ê²€ì‚¬ë¥¼ í†µí•´ ì§„ë‹¨ì„ ë°›ì•„ì•¼ í• ê¹Œìš”?     NaN    ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
      "4       HIV ê°ì—¼ ê²€ì‚¬ëŠ” ë°±í˜ˆêµ¬ ìˆ˜ì¹˜ ê°ì†Œë¡œ ì¸í•´ ê¶Œì¥ë˜ëŠ” ê²€ì‚¬ì¸ê°€ìš”?     NaN    ê°ì—¼ì„±ì§ˆí™˜  HIV ê°ì—¼   \n",
      "\n",
      "                                              source source_file subject  \\\n",
      "0  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...         NaN     NaN   \n",
      "1  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...         NaN     NaN   \n",
      "2  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...         NaN     NaN   \n",
      "3  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...         NaN     NaN   \n",
      "4  C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\120.ì´ˆê±°ëŒ€AI...         NaN     NaN   \n",
      "\n",
      "  sentence normalized_symptom department  \n",
      "0      NaN                NaN        NaN  \n",
      "1      NaN                NaN        NaN  \n",
      "2      NaN                NaN        NaN  \n",
      "3      NaN                NaN        NaN  \n",
      "4      NaN                NaN        NaN  \n",
      "['colloquial', 'answer', 'category', 'disease', 'source', 'source_file', 'subject', 'sentence', 'normalized_symptom', 'department']\n",
      "ë°ì´í„° ìˆ˜: 2625131\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\merged_all_data.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns.tolist())\n",
    "print(\"ë°ì´í„° ìˆ˜:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77df7efb-b367-4e26-9b8b-4d1aadbdd15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jang8\\AppData\\Local\\Temp\\ipykernel_15056\\2316680732.py:2: DtypeWarning: Columns (2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ê·œí™” í•™ìŠµ ë°ì´í„° ìˆ˜: 435\n",
      "                      text normalized_symptom department\n",
      "2624696    ë¨¸ë¦¬ê°€ ê¹¨ì§ˆ ê²ƒì²˜ëŸ¼ ì•„íŒŒìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "2624697   í•œìª½ ë¨¸ë¦¬ê°€ ì§€ëˆì§€ëˆ ì‘¤ì…”ìš”.                í¸ë‘í†µ        ì‹ ê²½ê³¼\n",
      "2624698  ë¨¸ë¦¬ ì „ì²´ê°€ ì¡°ì´ëŠ” ëŠë‚Œì´ì—ìš”.             ê¸´ì¥ì„± ë‘í†µ        ì‹ ê²½ê³¼\n",
      "2624699   ë¨¸ë¦¿ì†ì´ ìš¸ë¦¬ëŠ” ë“¯ì´ ì•„íŒŒìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "2624700  ì•„ì¹¨ë§ˆë‹¤ ë¨¸ë¦¬ê°€ ë¬´ê²ê³  ëµí•´ìš”.                 ë‘í†µ        ì‹ ê²½ê³¼\n",
      "normalized_symptom\n",
      "ë‘í†µ                    24\n",
      "normalized_symptom     8\n",
      "í•˜ë³µë¶€í†µì¦                  6\n",
      "ì–´ì§€ëŸ¼ì¦                   5\n",
      "ì†Œí™”ë¶ˆëŸ‰                   5\n",
      "í‰í†µ                     5\n",
      "ë³µí†µ                     5\n",
      "ë‡Œ ì•ˆê°œ                   4\n",
      "í¸ë‘í†µ                    4\n",
      "ê¸´ì¥ì„± ë‘í†µ                 4\n",
      "í˜¸í¡ê³¤ë€                   4\n",
      "ë§ì´ˆì‹ ê²½ë³‘ì¦                 4\n",
      "ë§Œì„± í”¼ë¡œ                  3\n",
      "ì´ëª…                     3\n",
      "ìˆ˜ë©´ì¥ì•                    3\n",
      "í‰ë¶€ ì••ë°•ê°                 3\n",
      "ê°€ë˜                     3\n",
      "ì‹¬ê³„í•­ì§„                   3\n",
      "ì¢Œê³¨ì‹ ê²½í†µ                  3\n",
      "ì§ˆì—¼                     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\jang8\\OneDrive\\ë°”íƒ• í™”ë©´\\ì¡¸ì‘\\ë°ì´í„°\\merged_all_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 1) í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì»¬ëŸ¼ìœ¼ë¡œ í†µí•©\n",
    "#    - Q&A ê³„ì—´: 'colloquial' ì±„ì›Œì ¸ ìˆìŒ\n",
    "#    - êµ¬ì–´ì²´ ì •ê·œí™” ê³„ì—´: 'sentence' ì±„ì›Œì ¸ ìˆìŒ\n",
    "df[\"text\"] = df[\"colloquial\"].fillna(df[\"sentence\"])\n",
    "\n",
    "# 2) normalized_symptom ì´ ìˆëŠ” í–‰ë§Œ ì‚¬ìš© (ì •ê·œí™” í•™ìŠµìš©)\n",
    "df_sym = df[~df[\"normalized_symptom\"].isna()].copy()\n",
    "\n",
    "# 3) í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°\n",
    "df_sym = df_sym[[\"text\", \"normalized_symptom\", \"department\"]]\n",
    "\n",
    "print(\"ì •ê·œí™” í•™ìŠµ ë°ì´í„° ìˆ˜:\", len(df_sym))\n",
    "print(df_sym.head())\n",
    "print(df_sym[\"normalized_symptom\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163d71c3-8665-4a00-a8d1-956b9d899c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ê°€ëŠ¥? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA ê°€ëŠ¥?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU ê°œìˆ˜:\", torch.cuda.device_count())\n",
    "    print(\"0ë²ˆ GPU ì´ë¦„:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecae7db-79cc-43fc-b437-2e8fa6aa3b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "torch.cuda version in torch: None\n",
      "GPU ê°œìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torch.cuda version in torch:\", torch.version.cuda)\n",
    "print(\"GPU ê°œìˆ˜:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a422277-703d-4913-b3d1-e194ff93b543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
